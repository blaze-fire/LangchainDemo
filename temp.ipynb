{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UcHxBDkTZ9T"
   },
   "outputs": [],
   "source": [
    "# !pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3BnosaURTbSo",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "faf7a924-fd7e-47f9-fe82-2ba179e44993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.38.32-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore<1.39.0,>=1.38.32 (from boto3)\n",
      "  Downloading botocore-1.38.32-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
      "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.32->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.32->boto3) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.32->boto3) (1.17.0)\n",
      "Downloading boto3-1.38.32-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.38.32-py3-none-any.whl (13.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.38.32 botocore-1.38.32 jmespath-1.0.1 s3transfer-0.13.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MAkHdRm5TgfL",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "6d15d9bd-f15e-4412-d7d4-fb7e4fd22858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
      "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.9.0 optuna-4.3.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "6O2xbM-GT8A_",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3a606449-e921-458c-fd7d-197177a1de0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
      "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: catboost\n",
      "Successfully installed catboost-1.2.8\n"
     ]
    }
   ],
   "source": [
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hNNiYCddSXiy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krish/miniconda3/envs/mlflow/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_auc_score\n",
    "# import boto3\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import joblib\n",
    "from mlflow.tracking import MlflowClient\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import optuna\n",
    "# from optuna.integration import MLflowCallback\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FdVDliwFTK6f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANjeyLSzTNnf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5h79ePDESZsz",
    "outputId": "0b3def36-f2da-42ae-c18a-f88b926997e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/09 11:09:42 INFO mlflow.tracking.fluent: Experiment with name 'ml-pipeline-versioning-experiment-1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/krish/Music/mlflow/mlruns/134065526802954240', creation_time=1749447582095, experiment_id='134065526802954240', last_update_time=1749447582095, lifecycle_stage='active', name='ml-pipeline-versioning-experiment-1', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup MLflow tracking\n",
    "mlflow.set_tracking_uri(\"./mlruns\")\n",
    "experiment_name = \"ml-pipeline-versioning-experiment-1\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5S6j26dKULF_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-J3eSSxUVQ7",
    "outputId": "4ea9b17d-9a1e-4a3a-dfe3-2e0938decf26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target target_names  \n",
      "0       0       setosa  \n",
      "1       0       setosa  \n",
      "2       0       setosa  \n",
      "3       0       setosa  \n",
      "4       0       setosa  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Add the target variable\n",
    "iris_df['target'] = iris.target\n",
    "\n",
    "# Add the target names\n",
    "iris_df['target_names'] = iris_df['target'].apply(lambda x: iris.target_names[x])\n",
    "\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zacWnCq0UVTf"
   },
   "outputs": [],
   "source": [
    "X= iris_df.drop(['target', 'target_names'], axis=1)\n",
    "y= iris_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "FM_TZBPAUVXC",
    "outputId": "cfdf4d34-71ad-4ad7-aa42-c450dcb48836"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nrfAtb7UK8R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kceiifl6SZvm"
   },
   "outputs": [],
   "source": [
    "class DataVersioningManager:\n",
    "    \"\"\"Manages data versioning and storage\"\"\"\n",
    "\n",
    "    def __init__(self, base_path=\"data_versions\"):\n",
    "        self.base_path = base_path\n",
    "        self.client = MlflowClient()\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    def create_data_version(self, data: pd.DataFrame, version_name: str = None,\n",
    "                           source: str = \"unknown\", tags: Dict = None) -> str:\n",
    "        \"\"\"Create a new data version\"\"\"\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        data_hash = self._calculate_data_hash(data)\n",
    "\n",
    "        if version_name is None:\n",
    "            version_name = f\"data_v_{timestamp}_{data_hash[:8]}\"\n",
    "\n",
    "        # Create data version metadata\n",
    "        data_metadata = {\n",
    "            \"version_name\": version_name,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"data_hash\": data_hash,\n",
    "            \"shape\": data.shape,\n",
    "            \"columns\": list(data.columns),\n",
    "            \"source\": source,\n",
    "            \"tags\": tags or {}\n",
    "        }\n",
    "\n",
    "        # Save data and metadata\n",
    "        data_path = os.path.join(self.base_path, f\"{version_name}.csv\")\n",
    "        metadata_path = os.path.join(self.base_path, f\"{version_name}_metadata.json\")\n",
    "\n",
    "        data.to_csv(data_path, index=False)\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(data_metadata, f, indent=2)\n",
    "\n",
    "        print(f\"✅ Data version created: {version_name}\")\n",
    "        print(f\"   Shape: {data.shape}\")\n",
    "        print(f\"   Hash: {data_hash[:16]}...\")\n",
    "        print(f\"   Source: {source}\")\n",
    "\n",
    "        return version_name\n",
    "\n",
    "    def load_data_version(self, version_name: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Load a specific data version\"\"\"\n",
    "\n",
    "        data_path = os.path.join(self.base_path, f\"{version_name}.csv\")\n",
    "        metadata_path = os.path.join(self.base_path, f\"{version_name}_metadata.json\")\n",
    "\n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Data version {version_name} not found\")\n",
    "\n",
    "        data = pd.read_csv(data_path)\n",
    "\n",
    "        if os.path.exists(metadata_path):\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "        else:\n",
    "            metadata = {}\n",
    "\n",
    "        return data, metadata\n",
    "\n",
    "    def list_data_versions(self) -> pd.DataFrame:\n",
    "        \"\"\"List all available data versions\"\"\"\n",
    "\n",
    "        versions = []\n",
    "        for file in os.listdir(self.base_path):\n",
    "            if file.endswith(\"_metadata.json\"):\n",
    "                with open(os.path.join(self.base_path, file), 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                    versions.append(metadata)\n",
    "\n",
    "        if versions:\n",
    "            return pd.DataFrame(versions)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def merge_data_versions(self, version_names: list, new_version_name: str = None) -> str:\n",
    "        \"\"\"Merge multiple data versions\"\"\"\n",
    "\n",
    "        combined_data = []\n",
    "        sources = []\n",
    "\n",
    "        for version_name in version_names:\n",
    "            data, metadata = self.load_data_version(version_name)\n",
    "            combined_data.append(data)\n",
    "            sources.append(metadata.get('source', version_name))\n",
    "\n",
    "        merged_data = pd.concat(combined_data, ignore_index=True)\n",
    "        merged_source = f\"merged_from_{'+'.join(sources)}\"\n",
    "\n",
    "        new_version = self.create_data_version(\n",
    "            merged_data,\n",
    "            new_version_name,\n",
    "            merged_source,\n",
    "            {\"merged_from\": version_names}\n",
    "        )\n",
    "\n",
    "        return new_version\n",
    "\n",
    "    def _calculate_data_hash(self, data: pd.DataFrame) -> str:\n",
    "        \"\"\"Calculate hash of data for versioning\"\"\"\n",
    "        return hashlib.md5(data.to_string().encode()).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rh3jmt3HSZyl"
   },
   "outputs": [],
   "source": [
    "class ModelVersioningManager:\n",
    "    \"\"\"Manages model versioning and registry\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"pipeline_classifier\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = MlflowClient()\n",
    "\n",
    "    def register_model_version(self, run_id: str, model_path: str = \"model\",\n",
    "                              description: str = None, tags: Dict = None) -> int:\n",
    "        \"\"\"Register a new model version\"\"\"\n",
    "\n",
    "        model_uri = f\"runs:/{run_id}/{model_path}\"\n",
    "\n",
    "        # Register model version\n",
    "        model_version = mlflow.register_model(\n",
    "            model_uri=model_uri,\n",
    "            name=self.model_name,\n",
    "            tags=tags\n",
    "        )\n",
    "\n",
    "        # Add description if provided\n",
    "        if description:\n",
    "            self.client.update_model_version(\n",
    "                name=self.model_name,\n",
    "                version=model_version.version,\n",
    "                description=description\n",
    "            )\n",
    "\n",
    "        print(f\"✅ Model version {model_version.version} registered\")\n",
    "        return int(model_version.version)\n",
    "\n",
    "    def get_latest_model_version(self) -> int:\n",
    "        \"\"\"Get the latest model version number\"\"\"\n",
    "        try:\n",
    "            latest_versions = self.client.get_latest_versions(\n",
    "                name=self.model_name,\n",
    "                stages=[\"None\", \"Staging\", \"Production\"]\n",
    "            )\n",
    "            if latest_versions:\n",
    "                return max([int(v.version) for v in latest_versions])\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def load_model_version(self, version: int = None):\n",
    "        \"\"\"Load a specific model version\"\"\"\n",
    "\n",
    "        if version is None:\n",
    "            version = self.get_latest_model_version()\n",
    "\n",
    "        model_uri = f\"models:/{self.model_name}/{version}\"\n",
    "        model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "        print(f\"✅ Loaded model version {version}\")\n",
    "        return model\n",
    "\n",
    "    def promote_model_version(self, version: int, stage: str):\n",
    "        \"\"\"Promote model version to a stage (Staging/Production)\"\"\"\n",
    "\n",
    "        self.client.transition_model_version_stage(\n",
    "            name=self.model_name,\n",
    "            version=version,\n",
    "            stage=stage\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Model version {version} promoted to {stage}\")\n",
    "\n",
    "    def list_model_versions(self) -> pd.DataFrame:\n",
    "        \"\"\"List all model versions\"\"\"\n",
    "\n",
    "        try:\n",
    "            versions = self.client.search_model_versions(f\"name='{self.model_name}'\")\n",
    "            version_data = []\n",
    "\n",
    "            for version in versions:\n",
    "                version_data.append({\n",
    "                    'version': version.version,\n",
    "                    'stage': version.current_stage,\n",
    "                    'creation_timestamp': version.creation_timestamp,\n",
    "                    'run_id': version.run_id,\n",
    "                    'description': version.description\n",
    "                })\n",
    "\n",
    "            return pd.DataFrame(version_data)\n",
    "        except:\n",
    "            return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4I8Ggbe0W5-y",
    "outputId": "7ba6a79d-5373-4899-9b15-d1aaf8680362"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X366T1hFW8Ti"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "lPLjbwjcSZ1Q"
   },
   "outputs": [],
   "source": [
    "class MLflowVersioningPipeline:\n",
    "    \"\"\"Enhanced MLflow Pipeline with Data and Model Versioning\"\"\"\n",
    "\n",
    "    def __init__(self, experiment_name=\"ml-pipeline-versioning-experiment\",\n",
    "                 model_name=\"pipeline_classifier\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model_name = model_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        self.num_classes = None\n",
    "        self.data_manager = DataVersioningManager()\n",
    "        self.model_manager = ModelVersioningManager(model_name)\n",
    "        self.artifact_path = \"pipeline_artifacts\"\n",
    "\n",
    "    def create_initial_data_version(self, data: pd.DataFrame, version_name: str = None) -> str:\n",
    "        \"\"\"Create initial data version\"\"\"\n",
    "\n",
    "        if version_name is None:\n",
    "            version_name = \"initial_training_data_v1\"\n",
    "\n",
    "        return self.data_manager.create_data_version(\n",
    "            data, version_name, \"initial_training\", {\"type\": \"training\"}\n",
    "        )\n",
    "\n",
    "    def _get_metric_function(self, metric_name: str, for_cv: bool = False):\n",
    "        \"\"\"Get metric function by name\"\"\"\n",
    "\n",
    "        metric_name = metric_name.lower()\n",
    "\n",
    "        average = 'binary' if self.num_classes == 2 else 'macro'\n",
    "\n",
    "        if metric_name == 'accuracy':\n",
    "            return make_scorer(accuracy_score) if for_cv else accuracy_score\n",
    "        elif metric_name == 'precision':\n",
    "            return make_scorer(precision_score, average=average) if for_cv else \\\n",
    "                  lambda y_true, y_pred: precision_score(y_true, y_pred, average=average)\n",
    "        elif metric_name == 'recall':\n",
    "            return make_scorer(recall_score, average=average) if for_cv else \\\n",
    "                  lambda y_true, y_pred: recall_score(y_true, y_pred, average=average)\n",
    "        elif metric_name == 'f1':\n",
    "            return make_scorer(f1_score, average=average) if for_cv else \\\n",
    "                  lambda y_true, y_pred: f1_score(y_true, y_pred, average=average)\n",
    "        elif metric_name == 'roc_auc':\n",
    "            return make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr') if for_cv else \\\n",
    "                  lambda y_true, y_prob: roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric: {metric_name}\")\n",
    "\n",
    "\n",
    "\n",
    "    def _create_model_instance(self, model_name: str, params: Dict = None):\n",
    "        \"\"\"Create model instance with parameters\"\"\"\n",
    "        model_classes = {\n",
    "            'randomforest': RandomForestClassifier,\n",
    "            'xgboost': xgb.XGBClassifier,\n",
    "            'lightgbm': lgb.LGBMClassifier,\n",
    "            'catboost': CatBoostClassifier\n",
    "        }\n",
    "\n",
    "        model_class = model_classes.get(model_name.lower())\n",
    "        if model_class is None:\n",
    "            raise ValueError(f\"Model {model_name} not supported\")\n",
    "\n",
    "        if params is None:\n",
    "            params = {}\n",
    "\n",
    "        # Handle CatBoost verbose parameter\n",
    "        if model_name.lower() == 'catboost':\n",
    "            params.setdefault('verbose', False)\n",
    "\n",
    "        return model_class(**params)\n",
    "\n",
    "    def _optimize_hyperparameters(self, X_train, y_train, model_name: str,\n",
    "                                param_space: Dict, metric: str, n_trials: int = 100):\n",
    "        \"\"\"Optimize hyperparameters using Optuna\"\"\"\n",
    "\n",
    "        def objective(trial):\n",
    "            # Sample parameters from the space\n",
    "            params = {}\n",
    "            for param_name, param_config in param_space.items():\n",
    "                if param_config['type'] == 'int':\n",
    "                    params[param_name] = trial.suggest_int(\n",
    "                        param_name, param_config['low'], param_config['high']\n",
    "                    )\n",
    "                elif param_config['type'] == 'float':\n",
    "                    params[param_name] = trial.suggest_float(\n",
    "                        param_name, param_config['low'], param_config['high']\n",
    "                    )\n",
    "                elif param_config['type'] == 'categorical':\n",
    "                    params[param_name] = trial.suggest_categorical(\n",
    "                        param_name, param_config['choices']\n",
    "                    )\n",
    "\n",
    "            # Create and train model\n",
    "            model = self._create_model_instance(model_name, params)\n",
    "\n",
    "            scorer = self._get_metric_function(metric, for_cv=True)\n",
    "\n",
    "            # Cross-validation score\n",
    "            scores = cross_val_score(\n",
    "                model, X_train, y_train,\n",
    "                cv=5, scoring=scorer\n",
    "            )\n",
    "\n",
    "            return scores.mean()\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "        return study.best_params, study.best_value\n",
    "\n",
    "    def run_training_pipeline(self, X_train, y_train, X_test, y_test,\n",
    "                             models_dict: Dict, hyperparams_dict: Dict = None,\n",
    "                             metric: str = 'accuracy', n_trials: int = 100,\n",
    "                             data_version: str = None, description: str = None) -> Dict:\n",
    "        \"\"\"Run training pipeline with model selection and hyperparameter tuning\"\"\"\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"training_pipeline_model_selection\"):\n",
    "            print(f\"🚀 Starting Training Pipeline with Model Selection\")\n",
    "\n",
    "            # Log basic info\n",
    "            mlflow.log_param(\"train_shape\", X_train.shape)\n",
    "            mlflow.log_param(\"test_shape\", X_test.shape)\n",
    "            mlflow.log_param(\"metric\", metric)\n",
    "            mlflow.log_param(\"n_trials\", n_trials)\n",
    "\n",
    "            self.num_classes = len(np.unique(y_train))\n",
    "\n",
    "            if data_version:\n",
    "                mlflow.log_param(\"data_version\", data_version)\n",
    "\n",
    "            # Preprocessing\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            best_model = None\n",
    "            best_score = -np.inf\n",
    "            best_model_name = None\n",
    "            best_params = None\n",
    "            model_results = {}\n",
    "\n",
    "            # Test each model\n",
    "            for model_name in models_dict.keys():\n",
    "                print(f\"\\n🔍 Testing model: {model_name}\")\n",
    "\n",
    "                # Get hyperparameter space for this model\n",
    "                param_space = hyperparams_dict.get(model_name, {}) if hyperparams_dict else {}\n",
    "\n",
    "                if param_space:\n",
    "                    # Optimize hyperparameters\n",
    "                    print(f\"   Optimizing hyperparameters with {n_trials} trials...\")\n",
    "                    best_model_params, best_cv_score = self._optimize_hyperparameters(\n",
    "                        X_train_scaled, y_train, model_name, param_space, metric, n_trials\n",
    "                    )\n",
    "                    print(f\"   Best CV score: {best_cv_score:.4f}\")\n",
    "                else:\n",
    "                    # Use default parameters\n",
    "                    best_model_params = {}\n",
    "                    best_cv_score = 0\n",
    "\n",
    "                # Train final model with best parameters\n",
    "                model = self._create_model_instance(model_name, best_model_params)\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "\n",
    "                # Evaluate\n",
    "                train_pred = model.predict(X_train_scaled)\n",
    "                test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "                metric_func = self._get_metric_function(metric, for_cv=False)\n",
    "\n",
    "                if metric.lower() == 'roc_auc':\n",
    "                    train_score = metric_func(y_train, model.predict_proba(X_train_scaled)[:, 1])\n",
    "                    test_score = metric_func(y_test, model.predict_proba(X_test_scaled)[:, 1])\n",
    "                else:\n",
    "                    train_score = metric_func(y_train, train_pred)\n",
    "                    test_score = metric_func(y_test, test_pred)\n",
    "\n",
    "                # Log model results\n",
    "                mlflow.log_param(f\"{model_name}_best_params\", best_model_params)\n",
    "                mlflow.log_metric(f\"{model_name}_train_{metric}\", train_score)\n",
    "                mlflow.log_metric(f\"{model_name}_test_{metric}\", test_score)\n",
    "\n",
    "                model_results[model_name] = {\n",
    "                    'model': model,\n",
    "                    'params': best_model_params,\n",
    "                    'train_score': train_score,\n",
    "                    'test_score': test_score,\n",
    "                    'cv_score': best_cv_score\n",
    "                }\n",
    "\n",
    "                print(f\"   Train {metric}: {train_score:.4f}\")\n",
    "                print(f\"   Test {metric}: {test_score:.4f}\")\n",
    "\n",
    "                # Check if this is the best model\n",
    "                if test_score > best_score:\n",
    "                    best_score = test_score\n",
    "                    best_model = model\n",
    "                    best_model_name = model_name\n",
    "                    best_params = best_model_params\n",
    "\n",
    "            print(f\"\\n🏆 Best model: {best_model_name} with {metric} = {best_score:.4f}\")\n",
    "\n",
    "            # Log best model info\n",
    "            mlflow.log_param(\"best_model\", best_model_name)\n",
    "            mlflow.log_param(\"best_params\", best_params)\n",
    "            mlflow.log_metric(f\"best_{metric}\", best_score)\n",
    "\n",
    "            # Retrain best model on full dataset\n",
    "            print(f\"\\n🔄 Retraining best model on full dataset...\")\n",
    "            X_full = np.vstack([X_train_scaled, X_test_scaled])\n",
    "            y_full = np.concatenate([y_train, y_test])\n",
    "\n",
    "            final_model = self._create_model_instance(best_model_name, best_params)\n",
    "            final_model.fit(X_full, y_full)\n",
    "\n",
    "            # Log final model\n",
    "            mlflow.sklearn.log_model(final_model, \"model\")\n",
    "\n",
    "            # Log scaler\n",
    "            joblib.dump(scaler, \"scaler.pkl\")\n",
    "            mlflow.log_artifact(\"scaler.pkl\", self.artifact_path)\n",
    "            os.remove(\"scaler.pkl\")\n",
    "\n",
    "            # Log model results summary\n",
    "            results_df = pd.DataFrame(model_results).T\n",
    "            results_df.to_csv(\"model_comparison.csv\")\n",
    "            mlflow.log_artifact(\"model_comparison.csv\", self.artifact_path)\n",
    "            os.remove(\"model_comparison.csv\")\n",
    "\n",
    "            # Register model version\n",
    "            run_id = mlflow.active_run().info.run_id\n",
    "            model_version = self.model_manager.register_model_version(\n",
    "                run_id,\n",
    "                description=description or f\"Best model: {best_model_name}\"\n",
    "            )\n",
    "\n",
    "            mlflow.log_param(\"model_version\", model_version)\n",
    "            mlflow.set_tag(\"model_version\", model_version)\n",
    "            mlflow.set_tag(\"best_model\", best_model_name)\n",
    "\n",
    "            print(f\"✅ Training completed!\")\n",
    "            print(f\"   Best Model: {best_model_name}\")\n",
    "            print(f\"   Model Version: {model_version}\")\n",
    "            print(f\"   Best {metric}: {best_score:.4f}\")\n",
    "\n",
    "            return {\n",
    "                'run_id': run_id,\n",
    "                'model_version': model_version,\n",
    "                'best_model_name': best_model_name,\n",
    "                'best_params': best_params,\n",
    "                'best_score': best_score,\n",
    "                'metric': metric,\n",
    "                'final_model': final_model,\n",
    "                'scaler': scaler,\n",
    "                'model_results': model_results\n",
    "            }\n",
    "\n",
    "    def make_predictions_with_logging(self, X_new: pd.DataFrame,\n",
    "                                     model_version: int = None,\n",
    "                                     log_predictions: bool = True) -> Tuple[np.ndarray, str]:\n",
    "        \"\"\"Make predictions and optionally log them for future retraining\"\"\"\n",
    "\n",
    "        # Load model\n",
    "        if model_version is None:\n",
    "            model_version = self.model_manager.get_latest_model_version()\n",
    "\n",
    "        model = self.model_manager.load_model_version(model_version)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_new)\n",
    "        probabilities = model.predict_proba(X_new)\n",
    "\n",
    "        # Log predictions if requested\n",
    "        prediction_data_version = None\n",
    "        if log_predictions:\n",
    "            # Create prediction data with features\n",
    "            pred_data = X_new.copy()\n",
    "            pred_data['prediction'] = predictions\n",
    "            pred_data['prediction_confidence'] = np.max(probabilities, axis=1)\n",
    "            pred_data['timestamp'] = datetime.now().isoformat()\n",
    "\n",
    "            # Create data version for predictions\n",
    "            prediction_data_version = self.data_manager.create_data_version(\n",
    "                pred_data,\n",
    "                source=f\"predictions_model_v{model_version}\",\n",
    "                tags={\"type\": \"predictions\", \"model_version\": model_version}\n",
    "            )\n",
    "\n",
    "        print(f\"✅ Made {len(predictions)} predictions using model version {model_version}\")\n",
    "        if prediction_data_version:\n",
    "            print(f\"   Predictions logged as data version: {prediction_data_version}\")\n",
    "\n",
    "        return predictions, prediction_data_version\n",
    "\n",
    "    def retrain_with_new_data(self, new_data: pd.DataFrame,\n",
    "                             include_prediction_data: bool = True,\n",
    "                             models_dict: Dict = None,\n",
    "                             hyperparams_dict: Dict = None,\n",
    "                             metric: str = 'accuracy') -> Dict:\n",
    "        \"\"\"Retrain model with new data combined with existing data\"\"\"\n",
    "\n",
    "        with mlflow.start_run(run_name=\"retraining_pipeline\"):\n",
    "            print(\"🔄 Starting Retraining Pipeline...\")\n",
    "\n",
    "            # Get all available data versions\n",
    "            available_versions = self.data_manager.list_data_versions()\n",
    "\n",
    "            if available_versions.empty:\n",
    "                print(\"⚠️  No previous data versions found. Creating initial version.\")\n",
    "                initial_version = self.create_initial_data_version(new_data)\n",
    "\n",
    "                # Prepare data for training\n",
    "                if 'target' in new_data.columns:\n",
    "                    X = new_data.drop('target', axis=1)\n",
    "                    y = new_data['target']\n",
    "                else:\n",
    "                    X = new_data.iloc[:, :-1]\n",
    "                    y = new_data.iloc[:, -1]\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.2, random_state=42, stratify=y\n",
    "                )\n",
    "\n",
    "                return self.run_training_pipeline(\n",
    "                    X_train, y_train, X_test, y_test,\n",
    "                    models_dict, hyperparams_dict, metric\n",
    "                )\n",
    "\n",
    "            # Create version for new data\n",
    "            new_data_version = self.data_manager.create_data_version(\n",
    "                new_data,\n",
    "                source=\"new_training_data\",\n",
    "                tags={\"type\": \"retraining\"}\n",
    "            )\n",
    "\n",
    "            # Get versions to merge\n",
    "            versions_to_merge = []\n",
    "\n",
    "            # Include original training data\n",
    "            training_versions = available_versions[\n",
    "                available_versions['tags'].apply(\n",
    "                    lambda x: isinstance(x, dict) and x.get('type') == 'training'\n",
    "                )\n",
    "            ]\n",
    "            if not training_versions.empty:\n",
    "                versions_to_merge.extend(training_versions['version_name'].tolist())\n",
    "\n",
    "            # Include prediction data if requested\n",
    "            if include_prediction_data:\n",
    "                prediction_versions = available_versions[\n",
    "                    available_versions['tags'].apply(\n",
    "                        lambda x: isinstance(x, dict) and x.get('type') == 'predictions'\n",
    "                    )\n",
    "                ]\n",
    "                if not prediction_versions.empty:\n",
    "                    versions_to_merge.extend(prediction_versions['version_name'].tolist())\n",
    "\n",
    "            # Add new data version\n",
    "            versions_to_merge.append(new_data_version)\n",
    "\n",
    "            # Merge all data versions\n",
    "            merged_version = self.data_manager.merge_data_versions(\n",
    "                versions_to_merge,\n",
    "                f\"merged_retraining_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            )\n",
    "\n",
    "            # Load merged data\n",
    "            merged_data, _ = self.data_manager.load_data_version(merged_version)\n",
    "\n",
    "            # Prepare data for training\n",
    "            if 'target' in merged_data.columns:\n",
    "                X = merged_data.drop('target', axis=1)\n",
    "                y = merged_data['target']\n",
    "            else:\n",
    "                X = merged_data.iloc[:, :-1]\n",
    "                y = merged_data.iloc[:, -1]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "\n",
    "            # Log retraining info\n",
    "            mlflow.log_param(\"retraining\", True)\n",
    "            mlflow.log_param(\"merged_versions\", len(versions_to_merge))\n",
    "            mlflow.log_param(\"versions_merged\", versions_to_merge)\n",
    "            mlflow.log_param(\"include_predictions\", include_prediction_data)\n",
    "\n",
    "            # Run training on merged data\n",
    "            training_result = self.run_training_pipeline(\n",
    "                X_train, y_train, X_test, y_test,\n",
    "                models_dict, hyperparams_dict, metric,\n",
    "                data_version=merged_version,\n",
    "                description=f\"Retrained on {len(versions_to_merge)} data versions\"\n",
    "            )\n",
    "\n",
    "            # Compare with previous model\n",
    "            previous_version = self.model_manager.get_latest_model_version() - 1\n",
    "            if previous_version > 0:\n",
    "                print(f\"📊 Comparing with previous model version {previous_version}\")\n",
    "                mlflow.log_param(\"previous_model_version\", previous_version)\n",
    "\n",
    "            print(f\"✅ Retraining completed!\")\n",
    "            print(f\"   New Model Version: {training_result['model_version']}\")\n",
    "            print(f\"   Data Versions Merged: {len(versions_to_merge)}\")\n",
    "\n",
    "            return training_result\n",
    "\n",
    "    def setup_feedback_loop(self, X_new: pd.DataFrame, y_true: np.ndarray = None,\n",
    "                           auto_retrain_threshold: float = 0.1) -> Dict:\n",
    "        \"\"\"Setup feedback loop for continuous learning\"\"\"\n",
    "\n",
    "        print(\"🔄 Setting up Feedback Loop...\")\n",
    "\n",
    "        # Make predictions\n",
    "        predictions, pred_data_version = self.make_predictions_with_logging(X_new)\n",
    "\n",
    "        results = {\n",
    "            'predictions': predictions,\n",
    "            'prediction_data_version': pred_data_version\n",
    "        }\n",
    "\n",
    "        # If ground truth is available, evaluate and potentially retrain\n",
    "        if y_true is not None:\n",
    "            current_accuracy = accuracy_score(y_true, predictions)\n",
    "            print(f\"📊 Current Model Accuracy: {current_accuracy:.4f}\")\n",
    "\n",
    "            # Get previous model performance\n",
    "            latest_version = self.model_manager.get_latest_model_version()\n",
    "            model_versions_df = self.model_manager.list_model_versions()\n",
    "\n",
    "            if not model_versions_df.empty:\n",
    "                # Get previous accuracy from run metrics\n",
    "                previous_run_id = model_versions_df[\n",
    "                    model_versions_df['version'] == str(latest_version)\n",
    "                ]['run_id'].iloc[0]\n",
    "\n",
    "                try:\n",
    "                    previous_run = mlflow.get_run(previous_run_id)\n",
    "                    previous_accuracy = previous_run.data.metrics.get('test_accuracy', 0)\n",
    "\n",
    "                    accuracy_drop = previous_accuracy - current_accuracy\n",
    "                    print(f\"📊 Previous Model Accuracy: {previous_accuracy:.4f}\")\n",
    "                    print(f\"📊 Accuracy Drop: {accuracy_drop:.4f}\")\n",
    "\n",
    "                    # Auto-retrain if performance drops significantly\n",
    "                    if accuracy_drop > auto_retrain_threshold:\n",
    "                        print(f\"🚨 Performance dropped by {accuracy_drop:.4f}, triggering retraining...\")\n",
    "\n",
    "                        # Create labeled data for retraining\n",
    "                        labeled_data = X_new.copy()\n",
    "                        labeled_data['target'] = y_true\n",
    "\n",
    "                        retrain_result = self.retrain_with_new_data(labeled_data)\n",
    "                        results.update(retrain_result)\n",
    "                        results['retraining_triggered'] = True\n",
    "                    else:\n",
    "                        print(\"✅ Model performance is stable, no retraining needed\")\n",
    "                        results['retraining_triggered'] = False\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️  Could not compare with previous model: {e}\")\n",
    "\n",
    "            results['current_accuracy'] = current_accuracy\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_pipeline_status(self) -> Dict:\n",
    "        \"\"\"Get comprehensive pipeline status\"\"\"\n",
    "\n",
    "        status = {}\n",
    "\n",
    "        # Data versions\n",
    "        data_versions = self.data_manager.list_data_versions()\n",
    "        status['data_versions'] = {\n",
    "            'total': len(data_versions),\n",
    "            'latest': data_versions.iloc[-1]['version_name'] if not data_versions.empty else None\n",
    "        }\n",
    "\n",
    "        # Model versions\n",
    "        model_versions = self.model_manager.list_model_versions()\n",
    "        status['model_versions'] = {\n",
    "            'total': len(model_versions),\n",
    "            'latest': self.model_manager.get_latest_model_version(),\n",
    "            'production': None\n",
    "        }\n",
    "\n",
    "        # Check for production model\n",
    "        if not model_versions.empty:\n",
    "            prod_models = model_versions[model_versions['stage'] == 'Production']\n",
    "            if not prod_models.empty:\n",
    "                status['model_versions']['production'] = prod_models.iloc[0]['version']\n",
    "\n",
    "        return status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpL2JZ3nf6HI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwJZ_YXca-kP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fRQfqCnQbM1x"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train , y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='test-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data version created: initial_production_training_data_v1\n",
      "   Shape: (150, 5)\n",
      "   Hash: 7b49656774eb8be2...\n",
      "   Source: initial_training\n",
      "Initial data version created: initial_production_training_data_v1\n"
     ]
    }
   ],
   "source": [
    "initial_data_version_name = pipeline.create_initial_data_version(\n",
    "    data=pd.concat([X, y], axis=1), # Combine X and y for data versioning\n",
    "    version_name=\"initial_production_training_data_v1\"\n",
    ")\n",
    "print(f\"Initial data version created: {initial_data_version_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CckGqePnSZ4D",
    "outputId": "0354c9ce-cf95-4913-e040-4069dce438cf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 11:57:44,350] A new study created in memory with name: no-name-4905d779-d0b2-4e84-822c-357195fba787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced MLflow Pipeline with Model Selection & Hyperparameter Tuning\n",
      "======================================================================\n",
      "\n",
      "✅ All examples completed!\n",
      "\n",
      "To use with your own data:\n",
      "🚀 Starting Training Pipeline with Model Selection\n",
      "\n",
      "🔍 Testing model: randomforest\n",
      "   Optimizing hyperparameters with 100 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 11:57:44,881] Trial 0 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 155, 'max_depth': 3}. Best is trial 0 with value: 0.9353479853479854.\n",
      "[I 2025-06-09 11:57:45,483] Trial 1 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 182, 'max_depth': 11}. Best is trial 0 with value: 0.9353479853479854.\n",
      "[I 2025-06-09 11:57:46,004] Trial 2 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 160, 'max_depth': 11}. Best is trial 0 with value: 0.9353479853479854.\n",
      "[I 2025-06-09 11:57:46,263] Trial 3 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 77, 'max_depth': 11}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:46,545] Trial 4 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 81, 'max_depth': 14}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:47,051] Trial 5 finished with value: 0.9349446958270488 and parameters: {'n_estimators': 154, 'max_depth': 3}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:47,725] Trial 6 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 198, 'max_depth': 13}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:48,280] Trial 7 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 171, 'max_depth': 4}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:48,857] Trial 8 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 175, 'max_depth': 6}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:49,199] Trial 9 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 102, 'max_depth': 10}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:49,423] Trial 10 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 64, 'max_depth': 7}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:49,858] Trial 11 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 123, 'max_depth': 15}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:50,310] Trial 12 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 126, 'max_depth': 12}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:50,982] Trial 13 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 199, 'max_depth': 13}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:51,170] Trial 14 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 50, 'max_depth': 8}. Best is trial 3 with value: 0.9443162393162392.\n",
      "[I 2025-06-09 11:57:51,484] Trial 15 finished with value: 0.9532051282051283 and parameters: {'n_estimators': 88, 'max_depth': 9}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:51,788] Trial 16 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 89, 'max_depth': 9}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:52,154] Trial 17 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 107, 'max_depth': 6}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:52,411] Trial 18 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 71, 'max_depth': 9}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:52,852] Trial 19 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 129, 'max_depth': 10}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:53,185] Trial 20 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 100, 'max_depth': 8}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:53,370] Trial 21 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 52, 'max_depth': 13}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:53,821] Trial 22 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 138, 'max_depth': 11}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:54,092] Trial 23 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 80, 'max_depth': 13}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:54,462] Trial 24 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 111, 'max_depth': 15}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:54,794] Trial 25 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 94, 'max_depth': 12}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:55,477] Trial 26 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 199, 'max_depth': 10}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:55,755] Trial 27 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 64, 'max_depth': 12}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:56,429] Trial 28 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 140, 'max_depth': 14}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:56,846] Trial 29 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 86, 'max_depth': 8}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:57,206] Trial 30 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 75, 'max_depth': 14}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:57,721] Trial 31 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 110, 'max_depth': 6}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:58,216] Trial 32 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 109, 'max_depth': 5}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:58,660] Trial 33 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 97, 'max_depth': 11}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:59,187] Trial 34 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 118, 'max_depth': 7}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:57:59,816] Trial 35 finished with value: 0.9532051282051283 and parameters: {'n_estimators': 145, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:00,614] Trial 36 finished with value: 0.9532051282051283 and parameters: {'n_estimators': 188, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:01,401] Trial 37 finished with value: 0.9349446958270488 and parameters: {'n_estimators': 186, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:02,085] Trial 38 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 161, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:02,711] Trial 39 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 147, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:03,478] Trial 40 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 182, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:04,278] Trial 41 finished with value: 0.9532051282051283 and parameters: {'n_estimators': 191, 'max_depth': 5}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:04,989] Trial 42 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 167, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:05,795] Trial 43 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 190, 'max_depth': 5}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:06,555] Trial 44 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 180, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:07,379] Trial 45 finished with value: 0.9349446958270488 and parameters: {'n_estimators': 193, 'max_depth': 5}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:08,089] Trial 46 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 168, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:08,746] Trial 47 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 157, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:09,373] Trial 48 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 149, 'max_depth': 5}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:10,186] Trial 49 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 178, 'max_depth': 7}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:10,528] Trial 50 finished with value: 0.9532051282051283 and parameters: {'n_estimators': 61, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:10,875] Trial 51 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 63, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:11,184] Trial 52 finished with value: 0.9532051282051283 and parameters: {'n_estimators': 58, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:11,497] Trial 53 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 59, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:11,878] Trial 54 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 72, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:12,349] Trial 55 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 86, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:12,610] Trial 56 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 51, 'max_depth': 6}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:12,903] Trial 57 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 57, 'max_depth': 5}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:13,531] Trial 58 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 132, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:13,854] Trial 59 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 68, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:14,809] Trial 60 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 193, 'max_depth': 5}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:15,201] Trial 61 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 80, 'max_depth': 9}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:15,485] Trial 62 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 58, 'max_depth': 10}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:15,845] Trial 63 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 78, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:16,164] Trial 64 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 67, 'max_depth': 12}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:16,953] Trial 65 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 175, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:17,401] Trial 66 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 88, 'max_depth': 8}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:17,682] Trial 67 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 55, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:18,154] Trial 68 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 73, 'max_depth': 9}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:18,819] Trial 69 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 118, 'max_depth': 7}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:19,397] Trial 70 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 93, 'max_depth': 11}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:20,563] Trial 71 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 196, 'max_depth': 13}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:21,463] Trial 72 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 187, 'max_depth': 15}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:22,406] Trial 73 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 186, 'max_depth': 11}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:23,321] Trial 74 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 199, 'max_depth': 12}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:23,808] Trial 75 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 102, 'max_depth': 14}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:24,649] Trial 76 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 172, 'max_depth': 14}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:24,950] Trial 77 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 63, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:25,295] Trial 78 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 76, 'max_depth': 10}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:26,115] Trial 79 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 190, 'max_depth': 6}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:26,496] Trial 80 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 83, 'max_depth': 13}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:26,982] Trial 81 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 105, 'max_depth': 6}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:27,439] Trial 82 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 93, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:27,983] Trial 83 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 113, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:29,224] Trial 84 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 164, 'max_depth': 5}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:29,970] Trial 85 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 123, 'max_depth': 8}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:30,638] Trial 86 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 134, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:30,999] Trial 87 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 70, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:31,701] Trial 88 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 146, 'max_depth': 5}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:31,983] Trial 89 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 61, 'max_depth': 6}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:32,915] Trial 90 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 200, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:33,868] Trial 91 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 138, 'max_depth': 10}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:34,718] Trial 92 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 127, 'max_depth': 11}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:35,658] Trial 93 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 151, 'max_depth': 12}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:36,350] Trial 94 finished with value: 0.9443162393162392 and parameters: {'n_estimators': 140, 'max_depth': 11}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:37,345] Trial 95 finished with value: 0.9532051282051283 and parameters: {'n_estimators': 196, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:38,287] Trial 96 finished with value: 0.9260558069381599 and parameters: {'n_estimators': 194, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:38,590] Trial 97 finished with value: 0.9353479853479854 and parameters: {'n_estimators': 54, 'max_depth': 5}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:39,383] Trial 98 finished with value: 0.9532051282051283 and parameters: {'n_estimators': 181, 'max_depth': 4}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:40,122] Trial 99 finished with value: 0.9349446958270488 and parameters: {'n_estimators': 181, 'max_depth': 3}. Best is trial 15 with value: 0.9532051282051283.\n",
      "[I 2025-06-09 11:58:40,209] A new study created in memory with name: no-name-ce401931-b8b9-4508-afe1-6154e44752f4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best CV score: 0.9532\n",
      "   Train f1: 1.0000\n",
      "   Test f1: 1.0000\n",
      "\n",
      "🔍 Testing model: xgboost\n",
      "   Optimizing hyperparameters with 100 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 11:58:40,489] Trial 0 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 200, 'learning_rate': 0.05590508409664613}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:40,650] Trial 1 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 102, 'learning_rate': 0.2186113040068244}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:40,816] Trial 2 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 101, 'learning_rate': 0.21365507393751168}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:41,006] Trial 3 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 103, 'learning_rate': 0.10559983973143014}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:41,260] Trial 4 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 162, 'learning_rate': 0.1446405230236645}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:41,549] Trial 5 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 200, 'learning_rate': 0.22233003319619563}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:41,859] Trial 6 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 159, 'learning_rate': 0.04679033261076263}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:42,128] Trial 7 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 155, 'learning_rate': 0.09948318538309497}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:42,360] Trial 8 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 139, 'learning_rate': 0.20441522749064986}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:42,559] Trial 9 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 116, 'learning_rate': 0.2883287211190031}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:42,712] Trial 10 finished with value: 0.9349446958270488 and parameters: {'n_estimators': 51, 'learning_rate': 0.021147273603095415}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:42,880] Trial 11 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 75, 'learning_rate': 0.2927316097618331}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:43,213] Trial 12 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 196, 'learning_rate': 0.15628332766920786}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:43,439] Trial 13 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 92, 'learning_rate': 0.07270936142654172}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:43,730] Trial 14 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 180, 'learning_rate': 0.16283440263045795}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:43,980] Trial 15 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 128, 'learning_rate': 0.24512308138116032}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:44,172] Trial 16 finished with value: 0.9349446958270488 and parameters: {'n_estimators': 74, 'learning_rate': 0.012677186413533222}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:44,418] Trial 17 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 137, 'learning_rate': 0.17772787036637255}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:44,718] Trial 18 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 177, 'learning_rate': 0.12535292453286623}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:44,919] Trial 19 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 77, 'learning_rate': 0.06598043152129016}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:45,125] Trial 20 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 114, 'learning_rate': 0.26270514448209625}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:45,317] Trial 21 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 91, 'learning_rate': 0.19747706823805633}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:45,770] Trial 22 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 102, 'learning_rate': 0.24597609458660244}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:46,005] Trial 23 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 116, 'learning_rate': 0.21512772252008655}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:46,159] Trial 24 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 59, 'learning_rate': 0.1885399306526809}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:46,358] Trial 25 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 91, 'learning_rate': 0.2303791838331018}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:46,598] Trial 26 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 143, 'learning_rate': 0.27132037067781156}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:46,815] Trial 27 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 105, 'learning_rate': 0.13387717437501429}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:46,995] Trial 28 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 82, 'learning_rate': 0.18008279519318937}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:47,254] Trial 29 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 130, 'learning_rate': 0.10630677872281205}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:47,481] Trial 30 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 104, 'learning_rate': 0.09689413440824214}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:47,865] Trial 31 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 109, 'learning_rate': 0.0471960654529382}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:48,075] Trial 32 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 97, 'learning_rate': 0.11712628491112571}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:48,323] Trial 33 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 121, 'learning_rate': 0.08433874552648223}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:48,530] Trial 34 finished with value: 0.9349446958270488 and parameters: {'n_estimators': 67, 'learning_rate': 0.04333747140115184}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:48,795] Trial 35 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 149, 'learning_rate': 0.1491846044714811}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:49,065] Trial 36 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 165, 'learning_rate': 0.22029259330655623}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:49,283] Trial 37 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 87, 'learning_rate': 0.059311943393384914}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:49,567] Trial 38 finished with value: 0.9349446958270488 and parameters: {'n_estimators': 123, 'learning_rate': 0.025318590169606314}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:49,903] Trial 39 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 193, 'learning_rate': 0.08584720560232736}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:50,176] Trial 40 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 166, 'learning_rate': 0.16950950726156344}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:50,495] Trial 41 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 199, 'learning_rate': 0.14765960593512675}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:50,787] Trial 42 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 188, 'learning_rate': 0.20940708929013602}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:51,061] Trial 43 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 157, 'learning_rate': 0.13626848205185568}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:51,350] Trial 44 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 180, 'learning_rate': 0.23894845855542582}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:51,566] Trial 45 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 97, 'learning_rate': 0.11120754105440964}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:51,806] Trial 46 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 137, 'learning_rate': 0.2014645484550226}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:52,107] Trial 47 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 172, 'learning_rate': 0.09529062401338237}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:52,413] Trial 48 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 187, 'learning_rate': 0.13868176738052038}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:52,619] Trial 49 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 110, 'learning_rate': 0.26025515754090495}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:52,804] Trial 50 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 84, 'learning_rate': 0.16226551627083177}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:53,092] Trial 51 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 188, 'learning_rate': 0.22510328385797043}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:53,391] Trial 52 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 200, 'learning_rate': 0.2777217504784389}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:53,694] Trial 53 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 193, 'learning_rate': 0.18722597510167566}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:53,886] Trial 54 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 97, 'learning_rate': 0.2345047851876395}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:54,159] Trial 55 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 171, 'learning_rate': 0.25016931442552687}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:54,450] Trial 56 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 182, 'learning_rate': 0.19365672510887616}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:54,668] Trial 57 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 116, 'learning_rate': 0.2104245493873528}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:54,892] Trial 58 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 128, 'learning_rate': 0.2999415001181932}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:55,087] Trial 59 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 77, 'learning_rate': 0.07009493398497635}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:55,339] Trial 60 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 147, 'learning_rate': 0.1809285371220883}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:55,676] Trial 61 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 156, 'learning_rate': 0.04829532813075963}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:55,916] Trial 62 finished with value: 0.9349446958270488 and parameters: {'n_estimators': 101, 'learning_rate': 0.02793807308600857}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:56,263] Trial 63 finished with value: 0.9349446958270488 and parameters: {'n_estimators': 163, 'learning_rate': 0.016468109692175922}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:56,561] Trial 64 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 131, 'learning_rate': 0.03422801694686524}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:56,861] Trial 65 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 175, 'learning_rate': 0.12257853672459228}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:57,096] Trial 66 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 108, 'learning_rate': 0.07594920490141865}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:57,445] Trial 67 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 193, 'learning_rate': 0.05843978409995927}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:57,683] Trial 68 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 113, 'learning_rate': 0.17054458520586657}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:57,928] Trial 69 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 142, 'learning_rate': 0.22432070816427965}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:58,201] Trial 70 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 121, 'learning_rate': 0.03680368680354643}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:58,653] Trial 71 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 152, 'learning_rate': 0.09601454752298844}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:58,960] Trial 72 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 163, 'learning_rate': 0.08111668118886006}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:59,305] Trial 73 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 184, 'learning_rate': 0.058222746186931205}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:58:59,909] Trial 74 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 93, 'learning_rate': 0.10588698047419941}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:00,159] Trial 75 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 135, 'learning_rate': 0.1278885788690253}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:00,436] Trial 76 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 160, 'learning_rate': 0.149979916587209}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:00,714] Trial 77 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 168, 'learning_rate': 0.2160144735947287}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:00,878] Trial 78 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 69, 'learning_rate': 0.25636477065326135}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:01,205] Trial 79 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 175, 'learning_rate': 0.06335252723164801}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:01,550] Trial 80 finished with value: 0.9349446958270488 and parameters: {'n_estimators': 152, 'learning_rate': 0.010118521832969345}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:01,803] Trial 81 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 145, 'learning_rate': 0.2054995824601682}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:02,060] Trial 82 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 156, 'learning_rate': 0.24232476348426124}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:02,302] Trial 83 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 140, 'learning_rate': 0.23023806224992197}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:02,635] Trial 84 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 196, 'learning_rate': 0.08955298516776969}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:02,781] Trial 85 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 50, 'learning_rate': 0.19155103372359225}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:02,987] Trial 86 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 89, 'learning_rate': 0.11028217061404726}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:03,241] Trial 87 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 150, 'learning_rate': 0.19902788727003307}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:03,447] Trial 88 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 107, 'learning_rate': 0.2163901781146987}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:03,648] Trial 89 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 94, 'learning_rate': 0.16067231969686496}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:03,850] Trial 90 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 103, 'learning_rate': 0.17197574988065695}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:04,085] Trial 91 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 118, 'learning_rate': 0.1414112766067306}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:04,292] Trial 92 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 111, 'learning_rate': 0.2511318364664628}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:04,516] Trial 93 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 124, 'learning_rate': 0.2920536696683847}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:04,748] Trial 94 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 134, 'learning_rate': 0.27680082015876745}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:04,992] Trial 95 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 101, 'learning_rate': 0.05121393040703786}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:05,195] Trial 96 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 106, 'learning_rate': 0.23584974734002923}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:05,493] Trial 97 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 196, 'learning_rate': 0.2692792013945081}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:05,750] Trial 98 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 119, 'learning_rate': 0.1294377662470651}. Best is trial 0 with value: 0.9442368742368743.\n",
      "[I 2025-06-09 11:59:05,948] Trial 99 finished with value: 0.9442368742368743 and parameters: {'n_estimators': 99, 'learning_rate': 0.20962278178678795}. Best is trial 0 with value: 0.9442368742368743.\n",
      "/home/krish/miniconda3/envs/mlflow/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/krish/miniconda3/envs/mlflow/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best CV score: 0.9442\n",
      "   Train f1: 1.0000\n",
      "   Test f1: 0.9484\n",
      "\n",
      "🔍 Testing model: lightgbm\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 88\n",
      "[LightGBM] [Info] Number of data points in the train set: 112, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -1.054937\n",
      "[LightGBM] [Info] Start training from score -1.134980\n",
      "[LightGBM] [Info] Start training from score -1.107581\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "   Train f1: 1.0000\n",
      "   Test f1: 0.9752\n",
      "\n",
      "🔍 Testing model: catboost\n",
      "   Train f1: 1.0000\n",
      "   Test f1: 1.0000\n",
      "\n",
      "🏆 Best model: randomforest with f1 = 1.0000\n",
      "\n",
      "🔄 Retraining best model on full dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/06/09 11:59:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model version 3 registered\n",
      "✅ Training completed!\n",
      "   Best Model: randomforest\n",
      "   Model Version: 3\n",
      "   Best f1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'test-model' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'test-model'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Enhanced MLflow Pipeline with Model Selection & Hyperparameter Tuning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run complete workflow example\n",
    "# pipeline, result = example_complete_workflow()\n",
    "\n",
    "# Run CatBoost example\n",
    "# catboost_result = example_with_catboost()\n",
    "\n",
    "# Run retraining workflow\n",
    "# initial_result, retrain_result = example_retraining_workflow()\n",
    "\n",
    "print(\"\\n✅ All examples completed!\")\n",
    "print(\"\\nTo use with your own data:\")\n",
    "\n",
    "# 1. Initialize pipeline\n",
    "pipeline = MLflowVersioningPipeline(\n",
    "    experiment_name=experiment_name,\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "# 2. Define models and hyperparameter spaces\n",
    "models_dict = {\n",
    "    'randomforest': RandomForestClassifier,\n",
    "    'xgboost': xgb.XGBClassifier,\n",
    "    'lightgbm': lgb.LGBMClassifier,\n",
    "    'catboost': CatBoostClassifier\n",
    "}\n",
    "\n",
    "hyperparams_dict = {\n",
    "    'randomforest': {\n",
    "        'n_estimators': {'type': 'int', 'low': 50, 'high': 200},\n",
    "        'max_depth': {'type': 'int', 'low': 3, 'high': 15}\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': {'type': 'int', 'low': 50, 'high': 200},\n",
    "        'learning_rate': {'type': 'float', 'low': 0.01, 'high': 0.3}\n",
    "    }\n",
    "    # Add more models and parameters as needed\n",
    "}\n",
    "\n",
    "# 3. Train with model selection\n",
    "result = pipeline.run_training_pipeline(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    models_dict=models_dict,\n",
    "    hyperparams_dict=hyperparams_dict,\n",
    "    metric='f1',  # or 'accuracy', 'precision', 'recall', 'roc_auc'\n",
    "    n_trials=100,\n",
    "    data_version=initial_data_version_name,\n",
    "    description=\"Training for initial production model selection\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "IGzJoswhjnTY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_id': '8d14f5c748d540e68e574ddaad8efc85',\n",
       " 'model_version': 3,\n",
       " 'best_model_name': 'randomforest',\n",
       " 'best_params': {'n_estimators': 88, 'max_depth': 9},\n",
       " 'best_score': 1.0,\n",
       " 'metric': 'f1',\n",
       " 'final_model': RandomForestClassifier(max_depth=9, n_estimators=88),\n",
       " 'scaler': StandardScaler(),\n",
       " 'model_results': {'randomforest': {'model': RandomForestClassifier(max_depth=9, n_estimators=88),\n",
       "   'params': {'n_estimators': 88, 'max_depth': 9},\n",
       "   'train_score': 1.0,\n",
       "   'test_score': 1.0,\n",
       "   'cv_score': 0.9532051282051283},\n",
       "  'xgboost': {'model': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                 colsample_bylevel=None, colsample_bynode=None,\n",
       "                 colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "                 enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                 feature_weights=None, gamma=None, grow_policy=None,\n",
       "                 importance_type=None, interaction_constraints=None,\n",
       "                 learning_rate=0.05590508409664613, max_bin=None,\n",
       "                 max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                 max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                 multi_strategy=None, n_estimators=200, n_jobs=None,\n",
       "                 num_parallel_tree=None, ...),\n",
       "   'params': {'n_estimators': 200, 'learning_rate': 0.05590508409664613},\n",
       "   'train_score': 1.0,\n",
       "   'test_score': 0.9483643892339545,\n",
       "   'cv_score': 0.9442368742368743},\n",
       "  'lightgbm': {'model': LGBMClassifier(),\n",
       "   'params': {},\n",
       "   'train_score': 1.0,\n",
       "   'test_score': 0.9751724137931035,\n",
       "   'cv_score': 0},\n",
       "  'catboost': {'model': <catboost.core.CatBoostClassifier at 0x7f6a59e1c4d0>,\n",
       "   'params': {'verbose': False},\n",
       "   'train_score': 1.0,\n",
       "   'test_score': 1.0,\n",
       "   'cv_score': 0}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training\n",
    "run_id = result['run_id']\n",
    "model_version_trained = result['model_version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Details for Run ID: 8d14f5c748d540e68e574ddaad8efc85\n",
      "  Data Version Used: initial_production_training_data_v1\n",
      "  Best Model: randomforest\n",
      "  Best Metric (f1): 1.0\n",
      "  Best Params: {'n_estimators': 88, 'max_depth': 9}\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "run = client.get_run(run_id)\n",
    "print(f\"\\nDetails for Run ID: {run_id}\")\n",
    "print(f\"  Data Version Used: {run.data.params.get('data_version')}\")\n",
    "print(f\"  Best Model: {run.data.params.get('best_model')}\")\n",
    "print(f\"  Best Metric ({run.data.params.get('metric')}): {run.data.metrics.get('best_f1')}\") # Or 'best_f1', etc.\n",
    "print(f\"  Best Params: {run.data.params.get('best_params')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Run: data=<RunData: metrics={'best_f1': 1.0,\n",
       " 'catboost_test_f1': 1.0,\n",
       " 'catboost_train_f1': 1.0,\n",
       " 'lightgbm_test_f1': 0.9751724137931035,\n",
       " 'lightgbm_train_f1': 1.0,\n",
       " 'randomforest_test_f1': 1.0,\n",
       " 'randomforest_train_f1': 1.0,\n",
       " 'xgboost_test_f1': 0.9483643892339545,\n",
       " 'xgboost_train_f1': 1.0}, params={'best_model': 'randomforest',\n",
       " 'best_params': \"{'n_estimators': 88, 'max_depth': 9}\",\n",
       " 'catboost_best_params': \"{'verbose': False}\",\n",
       " 'data_version': 'initial_production_training_data_v1',\n",
       " 'lightgbm_best_params': '{}',\n",
       " 'metric': 'f1',\n",
       " 'model_version': '3',\n",
       " 'n_trials': '100',\n",
       " 'randomforest_best_params': \"{'n_estimators': 88, 'max_depth': 9}\",\n",
       " 'test_shape': '(38, 4)',\n",
       " 'train_shape': '(112, 4)',\n",
       " 'xgboost_best_params': \"{'n_estimators': 200, 'learning_rate': \"\n",
       "                        '0.05590508409664613}'}, tags={'best_model': 'randomforest',\n",
       " 'mlflow.log-model.history': '[{\"run_id\": \"8d14f5c748d540e68e574ddaad8efc85\", '\n",
       "                             '\"artifact_path\": \"model\", \"utc_time_created\": '\n",
       "                             '\"2025-06-09 06:29:06.793302\", \"model_uuid\": '\n",
       "                             '\"5ad381883073400889cb8394f0f21761\", \"flavors\": '\n",
       "                             '{\"python_function\": {\"model_path\": \"model.pkl\", '\n",
       "                             '\"predict_fn\": \"predict\", \"loader_module\": '\n",
       "                             '\"mlflow.sklearn\", \"python_version\": \"3.11.0\", '\n",
       "                             '\"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": '\n",
       "                             '\"python_env.yaml\"}}, \"sklearn\": '\n",
       "                             '{\"pickled_model\": \"model.pkl\", '\n",
       "                             '\"sklearn_version\": \"1.7.0\", '\n",
       "                             '\"serialization_format\": \"cloudpickle\", \"code\": '\n",
       "                             'null}}}]',\n",
       " 'mlflow.runName': 'training_pipeline_model_selection',\n",
       " 'mlflow.source.name': '/home/krish/miniconda3/envs/mlflow/lib/python3.11/site-packages/ipykernel_launcher.py',\n",
       " 'mlflow.source.type': 'LOCAL',\n",
       " 'mlflow.user': 'krish',\n",
       " 'model_version': '3'}>, info=<RunInfo: artifact_uri='/home/krish/Music/mlflow/mlruns/134065526802954240/8d14f5c748d540e68e574ddaad8efc85/artifacts', end_time=1749450549391, experiment_id='134065526802954240', lifecycle_stage='active', run_id='8d14f5c748d540e68e574ddaad8efc85', run_name='training_pipeline_model_selection', run_uuid='8d14f5c748d540e68e574ddaad8efc85', start_time=1749450464343, status='FINISHED', user_id='krish'>, inputs=<RunInputs: dataset_inputs=[]>>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version_number = result['model_version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = f\"models:/your_production_classifier/{model_version_number}\"\n",
    "print(f\"Model URI to serve: {model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ5XQiCuSZ6t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSkarVLySZ-H"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ===== USAGE EXAMPLES =====\n",
    "\n",
    "def example_complete_workflow():\n",
    "    \"\"\"Complete workflow example with model selection and hyperparameter tuning\"\"\"\n",
    "\n",
    "    print(\"🎯 COMPLETE WORKFLOW WITH MODEL SELECTION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Initialize pipeline\n",
    "    pipeline = MLflowVersioningPipeline()\n",
    "\n",
    "    # 1. Create training data\n",
    "    print(\"\\n1️⃣ Creating Training Data...\")\n",
    "    X_train = pd.DataFrame({\n",
    "        'feature1': np.random.normal(0, 1, 800),\n",
    "        'feature2': np.random.normal(2, 1.5, 800),\n",
    "        'feature3': np.random.uniform(-1, 1, 800),\n",
    "        'feature4': np.random.exponential(1, 800)\n",
    "    })\n",
    "    y_train = np.random.choice([0, 1], 800, p=[0.6, 0.4])\n",
    "\n",
    "    X_test = pd.DataFrame({\n",
    "        'feature1': np.random.normal(0, 1, 200),\n",
    "        'feature2': np.random.normal(2, 1.5, 200),\n",
    "        'feature3': np.random.uniform(-1, 1, 200),\n",
    "        'feature4': np.random.exponential(1, 200)\n",
    "    })\n",
    "    y_test = np.random.choice([0, 1], 200, p=[0.6, 0.4])\n",
    "\n",
    "    # 2. Define models and hyperparameter spaces\n",
    "    models_dict = {\n",
    "        'randomforest': RandomForestClassifier,\n",
    "        'xgboost': xgb.XGBClassifier,\n",
    "        'lightgbm': lgb.LGBMClassifier\n",
    "    }\n",
    "\n",
    "    hyperparams_dict = {\n",
    "        'randomforest': {\n",
    "            'n_estimators': {'type': 'int', 'low': 50, 'high': 200},\n",
    "            'max_depth': {'type': 'int', 'low': 3, 'high': 15},\n",
    "            'min_samples_split': {'type': 'int', 'low': 2, 'high': 10}\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'n_estimators': {'type': 'int', 'low': 50, 'high': 200},\n",
    "            'max_depth': {'type': 'int', 'low': 3, 'high': 10},\n",
    "            'learning_rate': {'type': 'float', 'low': 0.01, 'high': 0.3},\n",
    "            'subsample': {'type': 'float', 'low': 0.6, 'high': 1.0}\n",
    "        },\n",
    "        'lightgbm': {\n",
    "            'n_estimators': {'type': 'int', 'low': 50, 'high': 200},\n",
    "            'max_depth': {'type': 'int', 'low': 3, 'high': 10},\n",
    "            'learning_rate': {'type': 'float', 'low': 0.01, 'high': 0.3},\n",
    "            'num_leaves': {'type': 'int', 'low': 10, 'high': 100}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 3. Train with model selection and hyperparameter tuning\n",
    "    print(\"\\n2️⃣ Training with Model Selection...\")\n",
    "    training_result = pipeline.run_training_pipeline(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        models_dict=models_dict,\n",
    "        hyperparams_dict=hyperparams_dict,\n",
    "        metric='f1',\n",
    "        n_trials=50\n",
    "    )\n",
    "\n",
    "    # 4. Make predictions\n",
    "    print(\"\\n3️⃣ Making Predictions...\")\n",
    "    new_data = pd.DataFrame({\n",
    "        'feature1': np.random.normal(0, 1, 100),\n",
    "        'feature2': np.random.normal(2, 1.5, 100),\n",
    "        'feature3': np.random.uniform(-1, 1, 100),\n",
    "        'feature4': np.random.exponential(1, 100)\n",
    "    })\n",
    "\n",
    "    predictions, pred_version = pipeline.make_predictions_with_logging(new_data)\n",
    "\n",
    "    # 5. Show pipeline status\n",
    "    print(\"\\n4️⃣ Pipeline Status...\")\n",
    "    status = pipeline.get_pipeline_status()\n",
    "    print(f\"Data Versions: {status['data_versions']['total']}\")\n",
    "    print(f\"Model Versions: {status['model_versions']['total']}\")\n",
    "    print(f\"Latest Model: v{status['model_versions']['latest']}\")\n",
    "    print(f\"Best Model: {training_result['best_model_name']}\")\n",
    "    print(f\"Best Score: {training_result['best_score']:.4f}\")\n",
    "\n",
    "    return pipeline, training_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1XgCCgTTIDO"
   },
   "outputs": [],
   "source": [
    "def example_with_catboost():\n",
    "    \"\"\"Example with CatBoost included\"\"\"\n",
    "\n",
    "    print(\"\\n🐱 EXAMPLE WITH CATBOOST\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    pipeline = MLflowVersioningPipeline()\n",
    "\n",
    "    # Create data\n",
    "    X_train = pd.DataFrame({\n",
    "        'feature1': np.random.normal(0, 1, 600),\n",
    "        'feature2': np.random.normal(2, 1.5, 600),\n",
    "        'feature3': np.random.uniform(-1, 1, 600)\n",
    "    })\n",
    "    y_train = np.random.choice([0, 1], 600, p=[0.7, 0.3])\n",
    "\n",
    "    X_test = pd.DataFrame({\n",
    "        'feature1': np.random.normal(0, 1, 150),\n",
    "        'feature2': np.random.normal(2, 1.5, 150),\n",
    "        'feature3': np.random.uniform(-1, 1, 150)\n",
    "    })\n",
    "    y_test = np.random.choice([0, 1], 150, p=[0.7, 0.3])\n",
    "\n",
    "    # Models with CatBoost\n",
    "    models_dict = {\n",
    "        'randomforest': RandomForestClassifier,\n",
    "        'xgboost': xgb.XGBClassifier,\n",
    "        'catboost': CatBoostClassifier\n",
    "    }\n",
    "\n",
    "    hyperparams_dict = {\n",
    "        'randomforest': {\n",
    "            'n_estimators': {'type': 'int', 'low': 50, 'high': 150},\n",
    "            'max_depth': {'type': 'int', 'low': 5, 'high': 15}\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'n_estimators': {'type': 'int', 'low': 50, 'high': 150},\n",
    "            'max_depth': {'type': 'int', 'low': 3, 'high': 8},\n",
    "            'learning_rate': {'type': 'float', 'low': 0.05, 'high': 0.25}\n",
    "        },\n",
    "        'catboost': {\n",
    "            'iterations': {'type': 'int', 'low': 50, 'high': 150},\n",
    "            'depth': {'type': 'int', 'low': 4, 'high': 8},\n",
    "            'learning_rate': {'type': 'float', 'low': 0.05, 'high': 0.25}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Train with ROC-AUC metric\n",
    "    result = pipeline.run_training_pipeline(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        models_dict=models_dict,\n",
    "        hyperparams_dict=hyperparams_dict,\n",
    "        metric='roc_auc',\n",
    "        n_trials=30\n",
    "    )\n",
    "\n",
    "    print(f\"Best Model: {result['best_model_name']}\")\n",
    "    print(f\"Best ROC-AUC: {result['best_score']:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SE1D-6Y-TFFi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3z4gY44U-MV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwc9-YM8VCu2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "id": "hgxdXHZXTBfS",
    "outputId": "44b8a5ed-00b5-4dae-a102-65482dc75a11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:13:36,335] A new study created in memory with name: no-name-ae6005bd-36e2-4d01-8ae0-71a68603e8c7\n",
      "[W 2025-06-07 03:13:36,338] Trial 0 failed with parameters: {'n_estimators': 155, 'max_depth': 14} because of the following error: TypeError(\"MLflowVersioningPipeline._get_metric_function() missing 1 required positional argument: 'y_train'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"<ipython-input-26-7e37e2acb723>\", line 87, in objective\n",
      "    cv=5, scoring=self._get_metric_function(metric)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: MLflowVersioningPipeline._get_metric_function() missing 1 required positional argument: 'y_train'\n",
      "[W 2025-06-07 03:13:36,340] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced MLflow Pipeline with Model Selection & Hyperparameter Tuning\n",
      "======================================================================\n",
      "\n",
      "✅ All examples completed!\n",
      "\n",
      "To use with your own data:\n",
      "🚀 Starting Training Pipeline with Model Selection\n",
      "\n",
      "🔍 Testing model: randomforest\n",
      "   Optimizing hyperparameters with 100 trials...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MLflowVersioningPipeline._get_metric_function() missing 1 required positional argument: 'y_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2b2441d8aa24>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# 3. Train with model selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m result = pipeline.run_training_pipeline(\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mmodels_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-7e37e2acb723>\u001b[0m in \u001b[0;36mrun_training_pipeline\u001b[0;34m(self, X_train, y_train, X_test, y_test, models_dict, hyperparams_dict, metric, n_trials, data_version, description)\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0;31m# Optimize hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Optimizing hyperparameters with {n_trials} trials...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                     best_model_params, best_cv_score = self._optimize_hyperparameters(\n\u001b[0m\u001b[1;32m    136\u001b[0m                         \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     )\n",
      "\u001b[0;32m<ipython-input-26-7e37e2acb723>\u001b[0m in \u001b[0;36m_optimize_hyperparameters\u001b[0;34m(self, X_train, y_train, model_name, param_space, metric, n_trials)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-7e37e2acb723>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     85\u001b[0m             scores = cross_val_score(\n\u001b[1;32m     86\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_metric_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             )\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: MLflowVersioningPipeline._get_metric_function() missing 1 required positional argument: 'y_train'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMbIM5wUVP7R"
   },
   "outputs": [],
   "source": [
    "# 4. Make predictions\n",
    "predictions, pred_version = pipeline.make_predictions_with_logging(X_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJ8BPGweVNsW"
   },
   "outputs": [],
   "source": [
    "# 5. Retrain with new data\n",
    "retrain_result = pipeline.retrain_with_new_data(\n",
    "    new_data,\n",
    "    models_dict=models_dict,\n",
    "    hyperparams_dict=hyperparams_dict,\n",
    "    metric='f1'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
